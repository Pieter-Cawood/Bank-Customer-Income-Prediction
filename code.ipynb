{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absa Corporate Client Activity Forecasting Challenge! \n",
    "\n",
    "<br> By Pieter Cawood\n",
    "\n",
    "This notebook implements weighted model averaging of 2 gradient boosting models. The first model is trained on quantile values  of selected transaction descriptions in the transaction data, along with the customer data. The second model is trained on quantile values of unique channel and product codes, along with the customer data. Both sets of features are supplemented with features of: the customer's transaction counts as well as quantile values of the amounts flowing into the customer's account. All features that are quantile values are determined for both the positive and negative amounts, that way the model should be able to learn from the user's earning and spending instead of simple average features.\n",
    "\n",
    "\n",
    "The models are partially trained on multiple cross validation sets, with overfit detection on each run. The predictions are finally made as a weighted sum of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (1.0.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from catboost->-r requirements.txt (line 1)) (0.20.1)\n",
      "Requirement already satisfied: six in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from catboost->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from catboost->-r requirements.txt (line 1)) (3.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from catboost->-r requirements.txt (line 1)) (1.9.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from catboost->-r requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2022.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow->-r requirements.txt (line 4)) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (63.4.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.11.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (23.1.21)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.31.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (15.0.6.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (3.19.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (1.51.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tqdm->-r requirements.txt (line 5)) (0.4.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from matplotlib->catboost->-r requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from matplotlib->catboost->-r requirements.txt (line 1)) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from matplotlib->catboost->-r requirements.txt (line 1)) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from matplotlib->catboost->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from matplotlib->catboost->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from plotly->catboost->-r requirements.txt (line 1)) (8.0.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.16.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pietercawood\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 4)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, GroupShuffleSplit\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "np.set_printoptions(suppress=True)\n",
    "# This seed is used by some data splitting and the models initial weights\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the data from file\n",
    "customer_df = pd.read_csv(\"Data/customer.csv\")\n",
    "income_group_df = pd.read_csv(\"Data/income_group.csv\")\n",
    "transactions_df = pd.read_csv(\"Data/transactions.csv\")\n",
    "train_incomes_df = pd.read_csv(\"Data/Train.csv\")\n",
    "test_incomes_df = pd.read_csv(\"Data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dates to np.datetime64\n",
    "transactions_df.RECORD_DATE = transactions_df.RECORD_DATE.astype(np.datetime64)\n",
    "# Make the comma seperated values as whole numbers\n",
    "train_incomes_df.DECLARED_NET_INCOME = train_incomes_df.DECLARED_NET_INCOME.str.replace(',','').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted lists of customer identifiers\n",
    "train_customer_identifiers = sorted(train_incomes_df.CUSTOMER_IDENTIFIER.unique())\n",
    "test_customer_identifiers = sorted(test_incomes_df.CUSTOMER_IDENTIFIER.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold transaction period\n",
    "Since the incomes might have changed over time, only transactions for the last 3 months within the data are considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_transactions = False\n",
    "\n",
    "if truncate_transactions:\n",
    "    transactions_horizon_days = 241\n",
    "    transaction_horizon_start = pd.Timestamp(transactions_df.RECORD_DATE.values.max() -\\\n",
    "                                             pd.Timedelta(transactions_horizon_days, 'd'))\n",
    "    print('Transactions horizon from ', transaction_horizon_start, ' to ', transactions_df.RECORD_DATE.values.max())\n",
    "    transactions_df = transactions_df[transactions_df.RECORD_DATE >= transaction_horizon_start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section handles customers with transaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions features\n",
    "Lets first analyse the top $n$ most common transaction types, to avoid using scarce transaction types. <br>\n",
    "The plan is to later use the lower quantile, median, upper quantile and max values of each transaction types for both <br>\n",
    "positive and negative amounts seperately, thus producing 8 features per $n$ defined transaction types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " 'POS PURCHASE',\n",
       " 'ATM WITHDRAWAL',\n",
       " 'AIRTIME DEBIT',\n",
       " 'ACB DEBIT:EXTERNAL',\n",
       " 'DIGITAL PAYMENT DT',\n",
       " 'ACB CREDIT',\n",
       " 'IBANK TRANSFER',\n",
       " 'LOTTO PURCHASE',\n",
       " 'CASHSEND DIGITAL',\n",
       " 'MANAGEMENT FEE',\n",
       " 'CHARGES',\n",
       " 'IMDTE DIGITAL PMT',\n",
       " 'DC TRACK INTERNAL',\n",
       " 'IBANK PAYMENT FROM',\n",
       " 'DC TRACK EXTERNAL',\n",
       " 'DIGITAL TRANSF DT',\n",
       " 'DIGITAL TRAN FEES',\n",
       " 'IMMEDIATE TRF CR',\n",
       " 'POS PUR & CASH',\n",
       " 'PREPAID DEBIT',\n",
       " 'INT DEBIT ORDER TO',\n",
       " 'REWARDS FEE',\n",
       " 'OVERSEAS PURCHASE',\n",
       " 'LOTTO WINNINGS',\n",
       " 'STOP ORDER TO',\n",
       " 'UNPAID DEBIT',\n",
       " 'CARDLESS CASH DEP',\n",
       " 'POS CASH WDL',\n",
       " 'TRI ATM WITHDRAWAL',\n",
       " 'OPENED-FROM SAV',\n",
       " 'TRANSFER FROM',\n",
       " 'DIGITAL TRANSF CR',\n",
       " 'CASH ACCEPTOR DEP',\n",
       " 'IBANK PAYMENT TO',\n",
       " 'EXT STOP ORDER TO',\n",
       " 'NPF CREDIT',\n",
       " 'CAN CASHSEND IB',\n",
       " 'INTEREST',\n",
       " 'ATM TRANSFER']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "number_of_transaction_types = 40\n",
    "\n",
    "transaction_counter = Counter(transactions_df.TRANSACTION_DESCRIPTION.values)\n",
    "transaction_feature_names = [name for name, count in transaction_counter.most_common(number_of_transaction_types)]\n",
    "transaction_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "<u> GBM model types #1</u> <br>\n",
    "Per customer count $n$ it takes the following shape $n \\ \\times \\ t \\ \\times \\ $ (pos amounts lower quantile, pos amounts median, pos amounts upper quantile and pos amounts max value, neg amounts lower quantile, neg amounts median, neg amounts upper quantile and neg amounts max value.) where $t$ is the number of used transaction types.\n",
    "\n",
    "<br> <u> GBM model types #2</u> <br>\n",
    "Per customer count $n$ it takes the following shape $n \\ \\times \\ f \\ \\times \\ $ (pos amounts lower quantile, pos amounts median, pos amounts upper quantile and pos amounts max value, neg amounts lower quantile, neg amounts median, neg amounts upper quantile and neg amounts max value.) where $f$ is the number of (unique channels + product codes.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_features_1(transcation_types, transactions_df_in, customer_identifiers, \n",
    "                               pos_quantiles=[0.25, 0.5, 0.75, 1.0],\n",
    "                               neg_quantiles=[0, 0.25, 0.5, 0.75]):\n",
    "    '''Gets the quantile values for each specified transaction type, and lastly an aggregation of \n",
    "    for all the other transaction types. The Positive and Negative values are kept as individual features.'''\n",
    "    all_user_features = []\n",
    "    for customer_id in tqdm(customer_identifiers):\n",
    "        user_transactions_df = transactions_df_in[transactions_df_in.CUSTOMER_IDENTIFIER == customer_id]\n",
    "        # Lets make the features for all of the selected transaction types \n",
    "        user_features = []\n",
    "        for transcation_type in transcation_types:\n",
    "            if type(transcation_type) is float:\n",
    "                # NaN transaction type\n",
    "                transaction_type_df = user_transactions_df[user_transactions_df.TRANSACTION_DESCRIPTION.isnull()]\n",
    "            else:\n",
    "                # Non-NaN transaction types\n",
    "                transaction_type_df = user_transactions_df[user_transactions_df.TRANSACTION_DESCRIPTION == transcation_type]\n",
    "            # First let's consider the positive amounts \n",
    "            pos_values = transaction_type_df[transaction_type_df.AMT > 0].AMT.values\n",
    "            # Now let's get the lower quantile, median, upper quantile and max for the postive amounts \n",
    "            if len(pos_values) == 0:\n",
    "                #No transactions, make zeros\n",
    "                user_features += [0] * len(pos_quantiles)\n",
    "            else:\n",
    "                #Get the quantile values\n",
    "                pos_qvals = np.quantile(pos_values, q=pos_quantiles).tolist()\n",
    "                user_features += pos_qvals\n",
    "            # Now let's consider the negative amounts\n",
    "            neg_values = transaction_type_df[transaction_type_df.AMT < 0].AMT.values\n",
    "            if len(neg_values) == 0:\n",
    "                #No transactions, make zeros\n",
    "                user_features += [0] * len(neg_quantiles)\n",
    "            else:\n",
    "                #Get the quantile values\n",
    "                neg_qvals = np.quantile(neg_values, q=neg_quantiles).tolist()\n",
    "                user_features += neg_qvals\n",
    "                \n",
    "        # Other transactions\n",
    "        other_transactions_df = user_transactions_df[~user_transactions_df.TRANSACTION_DESCRIPTION.isin(transcation_types)]\n",
    "        # First let's consider the positive amounts \n",
    "        pos_values = other_transactions_df[other_transactions_df.AMT > 0].AMT.values\n",
    "         # Now let's get the lower quantile, median, upper quantile and max for the postive amounts \n",
    "        if len(pos_values) == 0:\n",
    "            #No transactions, make zeros\n",
    "            user_features += [0] * len(pos_quantiles)\n",
    "        else:\n",
    "            #Get the quantile values\n",
    "            pos_qvals = np.quantile(pos_values, q=pos_quantiles).tolist()\n",
    "            user_features += pos_qvals\n",
    "        # Now let's consider the negative amounts\n",
    "        neg_values = other_transactions_df[other_transactions_df.AMT < 0].AMT.values\n",
    "        if len(neg_values) == 0:\n",
    "            #No transactions, make zeros\n",
    "            user_features += [0] * len(neg_quantiles)\n",
    "        else:\n",
    "            #Get the quantile values\n",
    "            neg_qvals = np.quantile(neg_values, q=neg_quantiles).tolist()\n",
    "            user_features += neg_qvals\n",
    "            \n",
    "        # Store all the features created for this user\n",
    "        all_user_features.append(user_features) \n",
    "        \n",
    "    return np.array(all_user_features).astype(float)     \n",
    "\n",
    "def get_transaction_features_2(transactions_df_in, customer_identifiers, \n",
    "                                  pos_quantiles=[0.25, 0.5, 0.75, 1.0],\n",
    "                                  neg_quantiles=[0, 0.25, 0.5, 0.75]):\n",
    "    '''Gets the quantile values for each unique channel and product code. The Positive and \n",
    "    Negative values are kept as individual features.'''\n",
    "    all_user_features = []\n",
    "    for customer_id in tqdm(customer_identifiers):\n",
    "        user_transactions_df = transactions_df_in[transactions_df_in.CUSTOMER_IDENTIFIER == customer_id]\n",
    "        user_features = []\n",
    "        for feature in (transactions_df.CHANNEL.unique().tolist() + transactions_df.PRODUCT_CODE.unique().tolist()):\n",
    "            pos_values = user_transactions_df[(user_transactions_df.CHANNEL == feature) &\\\n",
    "                                             (user_transactions_df.AMT > 0)].AMT.values\n",
    "            # No transactions of this type\n",
    "            if len(pos_values) == 0:\n",
    "                user_features += [0] * len(pos_quantiles)\n",
    "            else:\n",
    "                pos_qvals = np.quantile(pos_values, q=pos_quantiles).tolist()\n",
    "                user_features += pos_qvals\n",
    "                \n",
    "            neg_values = user_transactions_df[(user_transactions_df.CHANNEL == feature) &\\\n",
    "                                             (user_transactions_df.AMT < 0)].AMT.values\n",
    "            if len(neg_values) == 0:\n",
    "                user_features += [0] * len(neg_quantiles)\n",
    "            else:\n",
    "                neg_qvals = np.quantile(neg_values, q=neg_quantiles).tolist()\n",
    "                user_features += neg_qvals\n",
    "        # Store all the features created for this user\n",
    "        all_user_features.append(user_features)         \n",
    "    return np.array(all_user_features).astype(float)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Transaction counts </u> <br>\n",
    "Per customer count $n$ it prodives the features of positive and negative transaction counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_count_feature(transactions_df_in, customer_identifiers, \n",
    "                                  low_transaction_count_bins=[0, 5, 10, 20]):\n",
    "    '''Gets the number of transactions per customer,\n",
    "    This feature should help improve the modelling from quantiles. It returns the counts for \n",
    "    both the positive and negative transaction amounts.'''\n",
    "    pos_transaction_count = []\n",
    "    neg_transaction_count = []\n",
    "    transaction_count_category = []\n",
    "    for customer_id in tqdm(customer_identifiers):\n",
    "        pos_transaction_count.append(len(transactions_df_in[(transactions_df_in.CUSTOMER_IDENTIFIER == customer_id) &\\\n",
    "                                                            (transactions_df_in.AMT > 0)]))\n",
    "        neg_transaction_count.append(len(transactions_df_in[(transactions_df_in.CUSTOMER_IDENTIFIER == customer_id) &\\\n",
    "                                                            (transactions_df_in.AMT < 0)]))\n",
    "        \n",
    "        # Assign the user's transaction count to a binned feature\n",
    "        transaction_bin = 0\n",
    "        bin_assigned = False\n",
    "        for transaction_count in low_transaction_count_bins:\n",
    "            if (pos_transaction_count[-1] <= transaction_count)  or\\\n",
    "               (neg_transaction_count[-1] <= transaction_count):\n",
    "                    transaction_count_category.append(transaction_bin)\n",
    "                    bin_assigned = True\n",
    "                    break\n",
    "            transaction_bin += 1\n",
    "        # No bin assinged\n",
    "        if not bin_assigned:\n",
    "            transaction_count_category.append(len(low_transaction_count_bins))\n",
    "            \n",
    "    return np.array(pos_transaction_count), np.array(neg_transaction_count), np.array(transaction_count_category)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Account balance </u> <br>\n",
    "The customer's account balance qauntiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_account_balance_features(transactions_df_in, customer_identifiers, \n",
    "                                 quantiles=[0.25, 0.5, 0.75, 1.0]):\n",
    "    '''Gets the quantile values for each customer's account balance.'''\n",
    "    all_user_features = []\n",
    "    for customer_id in tqdm(customer_identifiers):\n",
    "        user_features = []\n",
    "        user_transactions_df = transactions_df_in[transactions_df_in.CUSTOMER_IDENTIFIER == customer_id]\n",
    "        # No transactions\n",
    "        if len(user_transactions_df) == 0:\n",
    "            user_features = [0] * len(quantiles)\n",
    "        else:\n",
    "            user_features = np.quantile(user_transactions_df.ACCOUNT_BALANCE.values, q=quantiles).tolist()\n",
    "        all_user_features.append(user_features)\n",
    "    return np.array(all_user_features).astype(float)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training transaction features\n",
    "This takes around 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3600/3600 [03:04<00:00, 19.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_balance_features = get_account_balance_features(transactions_df_in=transactions_df,\n",
    "                                                      customer_identifiers=train_customer_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3600/3600 [06:28<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_1_transaction_features = get_transaction_features_1(transcation_types=transaction_feature_names,\n",
    "                                                          transactions_df_in=transactions_df,\n",
    "                                                          customer_identifiers=train_customer_identifiers)\n",
    "\n",
    "train_1_transaction_features = np.hstack((train_1_transaction_features,\n",
    "                                         train_balance_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3600/3600 [06:59<00:00,  8.58it/s]\n"
     ]
    }
   ],
   "source": [
    "train_2_transaction_features = get_transaction_features_2(transactions_df_in=transactions_df,\n",
    "                                                          customer_identifiers=train_customer_identifiers)\n",
    "\n",
    "train_2_transaction_features = np.hstack((train_2_transaction_features,\n",
    "                                         train_balance_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3600/3600 [05:19<00:00, 11.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train_pos_transaction_counts, train_neg_transaction_counts,\\\n",
    "train_transaction_count_category = get_transaction_count_feature(transactions_df_in=transactions_df,\n",
    "                                                                 customer_identifiers=train_customer_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing transaction features\n",
    "This takes around 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1544/1544 [01:10<00:00, 21.80it/s]\n"
     ]
    }
   ],
   "source": [
    "test_balance_features = get_account_balance_features(transactions_df_in=transactions_df,\n",
    "                                                     customer_identifiers=test_customer_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1544/1544 [02:22<00:00, 10.85it/s]\n"
     ]
    }
   ],
   "source": [
    "test_1_transaction_features = get_transaction_features_1(transcation_types=transaction_feature_names,\n",
    "                                                         transactions_df_in=transactions_df,\n",
    "                                                         customer_identifiers=test_customer_identifiers)\n",
    "\n",
    "test_1_transaction_features = np.hstack((test_1_transaction_features,\n",
    "                                        test_balance_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1544/1544 [02:54<00:00,  8.87it/s]\n"
     ]
    }
   ],
   "source": [
    "test_2_transaction_features = get_transaction_features_2(transactions_df_in=transactions_df,\n",
    "                                                         customer_identifiers=test_customer_identifiers)\n",
    "\n",
    "test_2_transaction_features = np.hstack((test_2_transaction_features,\n",
    "                                        test_balance_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1544/1544 [02:23<00:00, 10.74it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pos_transaction_counts, test_neg_transaction_counts,\\\n",
    "test_transaction_count_category = get_transaction_count_feature(transactions_df_in=transactions_df,\n",
    "                                                                customer_identifiers=test_customer_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the customer group codes if outdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_to_group(value):\n",
    "    '''Helper function to return correct group code for a specified income.'''\n",
    "    if value < 500:\n",
    "        return 1\n",
    "    elif value < 1000:\n",
    "        return 2\n",
    "    elif value < 2000:\n",
    "        return 3\n",
    "    elif value < 3000:\n",
    "        return 4\n",
    "    elif value < 4000:\n",
    "        return 5\n",
    "    elif value < 5000:\n",
    "        return 6\n",
    "    elif value < 6000:\n",
    "        return 7\n",
    "    elif value < 7000:\n",
    "        return 8\n",
    "    elif value < 8000:\n",
    "        return 9\n",
    "    elif value < 9000:\n",
    "        return 10\n",
    "    elif value < 10000:\n",
    "        return 11\n",
    "    elif value < 12000:\n",
    "        return 12\n",
    "    elif value < 15000:\n",
    "        return 13\n",
    "    elif value < 20000:\n",
    "        return 14\n",
    "    elif value < 25000:\n",
    "        return 15\n",
    "    elif value < 34000:\n",
    "        return 16\n",
    "    elif value < 42000:\n",
    "        return 17\n",
    "    elif value < 63000:\n",
    "        return 18\n",
    "    elif value < 85000:\n",
    "        return 19\n",
    "    elif value < 125000:\n",
    "        return 20\n",
    "    else:\n",
    "        return 21\n",
    "\n",
    "def group_to_income(value):\n",
    "    '''Helper function to return average income for a specified group code.'''\n",
    "    if value < 2:\n",
    "        return 250\n",
    "    elif value < 3:\n",
    "        return 750\n",
    "    elif value < 4:\n",
    "        return 1500\n",
    "    elif value < 5:\n",
    "        return 2500\n",
    "    elif value < 6:\n",
    "        return 3500\n",
    "    elif value < 7:\n",
    "        return 4500\n",
    "    elif value < 8:\n",
    "        return 5500\n",
    "    elif value < 9:\n",
    "        return 6500\n",
    "    elif value < 10:\n",
    "        return 7500\n",
    "    elif value < 11:\n",
    "        return 8500\n",
    "    elif value < 12:\n",
    "        return 9500\n",
    "    elif value < 13:\n",
    "        return 11000\n",
    "    elif value < 14:\n",
    "        return 13500\n",
    "    elif value < 15:\n",
    "        return 17500\n",
    "    elif value < 16:\n",
    "        return 22500\n",
    "    elif value < 17:\n",
    "        return 29500\n",
    "    elif value < 18:\n",
    "        return 38000\n",
    "    elif value < 19:\n",
    "        return 52500\n",
    "    elif value < 20:\n",
    "        return 74000\n",
    "    else:\n",
    "        return 90000\n",
    "\n",
    "\n",
    "def get_customer_groups(customers_df_in, train_incomes_df_in):\n",
    "    '''Get updated user incomes according to the income codes csv.'''\n",
    "    \n",
    "    new_income_groups = []\n",
    "    new_update_dates = []\n",
    "    old_df = customers_df_in.copy()\n",
    "    train_incomes_df_ = train_incomes_df_in.copy()\n",
    "    old_df['RECORD_DATE'] = train_incomes_df_.RECORD_DATE\n",
    "    old_df['NET_INCOME'] = train_incomes_df_.DECLARED_NET_INCOME\n",
    "    for income_code, net_income, record_date, last_update in\\\n",
    "        zip(old_df.INCOME_GROUP_CODE.values, old_df.NET_INCOME.values, \n",
    "            old_df.RECORD_DATE.values, old_df.DATE_LAST_UPDATED.values):\n",
    "        if record_date > last_update:\n",
    "            # Outdates user, get an updated income group code.\n",
    "            new_income_groups.append(income_to_group(net_income))\n",
    "            new_update_dates.append(record_date)\n",
    "        else:\n",
    "            # User is up to date, use the known income group code.\n",
    "            new_income_groups.append(income_code)\n",
    "            new_update_dates.append(last_update)  \n",
    "    return new_income_groups, new_update_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_dataset(customer_features, transaction_features, categorical_indices):\n",
    "    '''Combines features and formats them for modelling. If targets are given the data is split into two subsets of data,\n",
    "    to train two different models to ensemble.'''\n",
    "    # Combine customer features and transactional ones\n",
    "    stacked_features = np.hstack((customer_features, transaction_features))\n",
    "    # No targets were given, this is probably test data\n",
    "    x = pd.DataFrame(stacked_features)\n",
    "    # Make the categorical features of type int\n",
    "    if categorical_indices is not None:\n",
    "        for index in categorical_indices: \n",
    "            x[index] = x[index].astype(int)\n",
    "    return x\n",
    "def get_customer_groups(customers_df_in, train_incomes_df_in):\n",
    "    '''Get updated user incomes according to the income codes csv.'''\n",
    "    \n",
    "    new_income_groups = []\n",
    "    new_update_dates = []\n",
    "    old_df = customers_df_in.copy()\n",
    "    train_incomes_df_ = train_incomes_df_in.copy()\n",
    "    old_df['RECORD_DATE'] = train_incomes_df_.RECORD_DATE\n",
    "    old_df['NET_INCOME'] = train_incomes_df_.DECLARED_NET_INCOME\n",
    "    for income_code, net_income, record_date, last_update in\\\n",
    "        zip(old_df.INCOME_GROUP_CODE.values, old_df.NET_INCOME.values, \n",
    "            old_df.RECORD_DATE.values, old_df.DATE_LAST_UPDATED.values):\n",
    "        if record_date > last_update:\n",
    "            # Outdates user, get an updated income group code.\n",
    "            new_income_groups.append(income_to_group(net_income))\n",
    "            new_update_dates.append(record_date)\n",
    "        else:\n",
    "            # User is up to date, use the known income group code.\n",
    "            new_income_groups.append(income_code)\n",
    "            new_update_dates.append(last_update)  \n",
    "    return new_income_groups, new_update_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_dataset(customer_features, transaction_features, categorical_indices):\n",
    "    '''Combines features and formats them for modelling. If targets are given the data is split into two subsets of data,\n",
    "    to train two different models to ensemble.'''\n",
    "    # Combine customer features and transactional ones\n",
    "    stacked_features = np.hstack((customer_features, transaction_features))\n",
    "    # No targets were given, this is probably test data\n",
    "    x = pd.DataFrame(stacked_features)\n",
    "    # Make the categorical features of type int\n",
    "    if categorical_indices is not None:\n",
    "        for index in categorical_indices: \n",
    "            x[index] = x[index].astype(int)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the customer dataframes for the modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_customer_features = customer_df.copy()\n",
    "train_customer_features = train_customer_features[train_customer_features.\\\n",
    "                                                 CUSTOMER_IDENTIFIER.isin(train_customer_identifiers)]\n",
    "train_customer_features = train_customer_features.sort_values(by=\"CUSTOMER_IDENTIFIER\")\n",
    "train_customer_features[\"TRANSACTION_POS_COUNTS\"] = train_pos_transaction_counts\n",
    "train_customer_features[\"TRANSACTION_NEG_COUNTS\"] = train_neg_transaction_counts\n",
    "train_customer_features[\"TRANSACTION_COUNT_CATEGORY\"] = train_transaction_count_category\n",
    "train_customer_features.DATE_LAST_UPDATED = train_customer_features.DATE_LAST_UPDATED.astype(np.datetime64)\n",
    "\n",
    "# Update the income groups and last updated\n",
    "train_incomes_df.RECORD_DATE = train_incomes_df.RECORD_DATE.astype(np.datetime64)\n",
    "train_incomes_df = train_incomes_df.sort_values(by=\"CUSTOMER_IDENTIFIER\")\n",
    "new_train_income_groups, _ = get_customer_groups(train_customer_features, train_incomes_df)\n",
    "train_customer_features['INCOME_GROUP_CODE'] = new_train_income_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_customer_features = customer_df.copy()\n",
    "test_customer_features = test_customer_features[test_customer_features.CUSTOMER_IDENTIFIER.isin(test_customer_identifiers)]\n",
    "test_customer_features = test_customer_features.sort_values(by=\"CUSTOMER_IDENTIFIER\")\n",
    "test_customer_features[\"TRANSACTION_POS_COUNTS\"] = test_pos_transaction_counts\n",
    "test_customer_features[\"TRANSACTION_NEG_COUNTS\"] = test_neg_transaction_counts\n",
    "test_customer_features[\"TRANSACTION_COUNT_CATEGORY\"] = test_transaction_count_category\n",
    "test_customer_features.DATE_LAST_UPDATED = test_customer_features.DATE_LAST_UPDATED.astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop high cardinality and unused features\n",
    "columns_to_drop = [\"CUSTOMER_IDENTIFIER\", \"DATE_LAST_UPDATED\", \"SEX_CODE\", \"OCCUPATIONAL_STATUS_CODE\"]\n",
    "train_customer_features.drop(columns=columns_to_drop, inplace=True)\n",
    "test_customer_features.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"TRANSACTION_COUNT_CATEGORY\"]\n",
    "categorical_feature_indices = [train_customer_features.columns.get_loc(feature) for feature in categorical_features]\n",
    "\n",
    "categorical_feature_indices = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the final train and test ready data sets\n",
    "\n",
    "There are 2 types of data structures used. GBM type 1 and GBM type 2. The following code just joins the customer and transaction features and optionally splits them so that we can train an ensemble of GBMs instead of a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_incomes_df[train_incomes_df.CUSTOMER_IDENTIFIER.isin(train_customer_identifiers)].\\\n",
    "                            sort_values(by=\"CUSTOMER_IDENTIFIER\").DECLARED_NET_INCOME.values\n",
    "\n",
    "train_x_1 = make_dataset(train_customer_features.values,\n",
    "                         train_1_transaction_features, \n",
    "                         categorical_feature_indices)\n",
    "\n",
    "train_x_2 = make_dataset(train_customer_features.values,\n",
    "                         train_2_transaction_features, \n",
    "                         categorical_feature_indices)\n",
    "\n",
    "test_x_1 = make_dataset(test_customer_features.values, \n",
    "                        test_1_transaction_features, \n",
    "                        categorical_feature_indices)\n",
    "\n",
    "test_x_2 = make_dataset(test_customer_features.values, \n",
    "                        test_2_transaction_features, \n",
    "                        categorical_feature_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Models\n",
    "CatBoost seems to work best, there are 3 models per data structure, the models utilising the structure type #1 are referred to as mdl1 and the models that utilise the structure type #2 are referred to as mdl2.\n",
    "Each mdlx_0 learns from both the subsets of data, where the mdlx_1 & mdlx_2 models are trained using a single subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Fold 1\n",
      "0:\tlearn: 10242.7847738\ttest: 9298.9161984\tbest: 9298.9161984 (0)\ttotal: 171ms\tremaining: 8m 31s\n",
      "100:\tlearn: 7087.3481089\ttest: 6625.6398699\tbest: 6625.6398699 (100)\ttotal: 1.13s\tremaining: 32.5s\n",
      "200:\tlearn: 6499.2693253\ttest: 6365.2688913\tbest: 6361.9977010 (198)\ttotal: 2.3s\tremaining: 32.1s\n",
      "300:\tlearn: 6238.7586918\ttest: 6266.5385236\tbest: 6266.5385236 (300)\ttotal: 3.73s\tremaining: 33.4s\n",
      "400:\tlearn: 6020.9125420\ttest: 6217.8387983\tbest: 6217.8259973 (391)\ttotal: 4.98s\tremaining: 32.3s\n",
      "500:\tlearn: 5735.7042481\ttest: 6168.4244534\tbest: 6167.0223603 (496)\ttotal: 6.17s\tremaining: 30.8s\n",
      "600:\tlearn: 5420.2279354\ttest: 6131.7948344\tbest: 6131.7948344 (600)\ttotal: 7.49s\tremaining: 29.9s\n",
      "700:\tlearn: 5153.1199635\ttest: 6114.2430578\tbest: 6107.7755645 (677)\ttotal: 8.78s\tremaining: 28.8s\n",
      "800:\tlearn: 4944.9595800\ttest: 6096.0340620\tbest: 6096.0340620 (800)\ttotal: 10.2s\tremaining: 28s\n",
      "900:\tlearn: 4749.4231776\ttest: 6063.5393145\tbest: 6060.7402755 (881)\ttotal: 11.5s\tremaining: 26.9s\n",
      "1000:\tlearn: 4576.2946661\ttest: 6041.3211362\tbest: 6041.3211362 (1000)\ttotal: 12.8s\tremaining: 25.6s\n",
      "1100:\tlearn: 4411.9461028\ttest: 6006.2333411\tbest: 6004.2085233 (1098)\ttotal: 14.2s\tremaining: 24.5s\n",
      "1200:\tlearn: 4275.7553239\ttest: 5992.3835409\tbest: 5992.3155179 (1196)\ttotal: 15.7s\tremaining: 23.5s\n",
      "1300:\tlearn: 4144.9730672\ttest: 5978.6096990\tbest: 5978.6096990 (1300)\ttotal: 17.4s\tremaining: 22.8s\n",
      "1400:\tlearn: 4013.6044042\ttest: 5972.2787108\tbest: 5972.2177108 (1399)\ttotal: 19s\tremaining: 21.7s\n",
      "1500:\tlearn: 3900.6311295\ttest: 5984.3785378\tbest: 5971.4007835 (1406)\ttotal: 20.4s\tremaining: 20.4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 5971.400784\n",
      "bestIteration = 1406\n",
      "\n",
      "Shrink model to first 1407 iterations.\n",
      "Model 1: Fold 2\n",
      "0:\tlearn: 4187.0180172\ttest: 4730.3824259\tbest: 4730.3824259 (0)\ttotal: 13.9ms\tremaining: 41.6s\n",
      "100:\tlearn: 4129.7046875\ttest: 4736.0621386\tbest: 4730.2634912 (30)\ttotal: 1.44s\tremaining: 41.3s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4730.263491\n",
      "bestIteration = 30\n",
      "\n",
      "Shrink model to first 31 iterations.\n",
      "Model 1: Fold 3\n",
      "0:\tlearn: 4245.6475108\ttest: 4078.6947358\tbest: 4078.6947358 (0)\ttotal: 14.9ms\tremaining: 44.8s\n",
      "100:\tlearn: 4194.5142242\ttest: 4085.5998239\tbest: 4078.6947358 (0)\ttotal: 1.33s\tremaining: 38.3s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4078.694736\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 1: Fold 4\n",
      "0:\tlearn: 4262.9140496\ttest: 3907.1799039\tbest: 3907.1799039 (0)\ttotal: 13.3ms\tremaining: 39.9s\n",
      "100:\tlearn: 4211.0851803\ttest: 3915.6886449\tbest: 3907.1327517 (2)\ttotal: 1.28s\tremaining: 36.6s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 3907.132752\n",
      "bestIteration = 2\n",
      "\n",
      "Shrink model to first 3 iterations.\n",
      "Model 1: Fold 5\n",
      "0:\tlearn: 4289.8075554\ttest: 3619.2016454\tbest: 3619.2016454 (0)\ttotal: 13ms\tremaining: 39s\n",
      "100:\tlearn: 4236.7296399\ttest: 3631.7340329\tbest: 3619.2016454 (0)\ttotal: 1.28s\tremaining: 36.7s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 3619.201645\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 1: Fold 6\n",
      "0:\tlearn: 4285.8289485\ttest: 3655.9654454\tbest: 3655.9654454 (0)\ttotal: 17.8ms\tremaining: 53.5s\n",
      "100:\tlearn: 4234.9895277\ttest: 3667.5403568\tbest: 3655.7596665 (4)\ttotal: 1.28s\tremaining: 36.6s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 3655.759666\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "Model 1: Fold 7\n",
      "0:\tlearn: 4164.8437979\ttest: 4734.1425692\tbest: 4734.1425692 (0)\ttotal: 14.9ms\tremaining: 44.6s\n",
      "100:\tlearn: 4113.3153325\ttest: 4742.6903009\tbest: 4734.1425692 (0)\ttotal: 1.23s\tremaining: 35.3s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4734.142569\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 1: Fold 8\n",
      "0:\tlearn: 4225.3279778\ttest: 4219.4470735\tbest: 4219.4470735 (0)\ttotal: 11ms\tremaining: 33s\n",
      "100:\tlearn: 4175.2482381\ttest: 4226.0559393\tbest: 4219.2852380 (4)\ttotal: 1.24s\tremaining: 35.5s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4219.285238\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "Model 1: Fold 9\n",
      "0:\tlearn: 4281.7370426\ttest: 3651.3027957\tbest: 3651.3027957 (0)\ttotal: 11.8ms\tremaining: 35.4s\n",
      "100:\tlearn: 4232.6670332\ttest: 3664.5446701\tbest: 3651.3027957 (0)\ttotal: 1.23s\tremaining: 35.3s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 3651.302796\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 1: Fold 10\n",
      "0:\tlearn: 4329.8316542\ttest: 3093.0523543\tbest: 3093.0523543 (0)\ttotal: 13.3ms\tremaining: 40s\n",
      "100:\tlearn: 4277.5541700\ttest: 3107.0339187\tbest: 3093.0523543 (0)\ttotal: 1.18s\tremaining: 33.8s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 3093.052354\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 2: Fold 1\n",
      "0:\tlearn: 10245.9745413\ttest: 9296.0324368\tbest: 9296.0324368 (0)\ttotal: 5.34ms\tremaining: 16s\n",
      "100:\tlearn: 6662.0496045\ttest: 6293.4491475\tbest: 6293.4491475 (100)\ttotal: 479ms\tremaining: 13.7s\n",
      "200:\tlearn: 6062.6141260\ttest: 5958.4409006\tbest: 5958.4409006 (200)\ttotal: 949ms\tremaining: 13.2s\n",
      "300:\tlearn: 5788.8536281\ttest: 5872.2311803\tbest: 5872.2311803 (300)\ttotal: 1.45s\tremaining: 13s\n",
      "400:\tlearn: 5546.1273937\ttest: 5839.3993868\tbest: 5838.7988490 (395)\ttotal: 1.98s\tremaining: 12.8s\n",
      "500:\tlearn: 5270.3448647\ttest: 5845.7022749\tbest: 5837.0872745 (422)\ttotal: 2.49s\tremaining: 12.4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 5837.087275\n",
      "bestIteration = 422\n",
      "\n",
      "Shrink model to first 423 iterations.\n",
      "Model 2: Fold 2\n",
      "0:\tlearn: 5489.8063855\ttest: 5803.4179593\tbest: 5803.4179593 (0)\ttotal: 5.07ms\tremaining: 15.2s\n",
      "100:\tlearn: 5298.1093090\ttest: 5822.2795739\tbest: 5802.4933150 (3)\ttotal: 445ms\tremaining: 12.8s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 5802.493315\n",
      "bestIteration = 3\n",
      "\n",
      "Shrink model to first 4 iterations.\n",
      "Model 2: Fold 3\n",
      "0:\tlearn: 5415.9675943\ttest: 6333.4348702\tbest: 6333.4348702 (0)\ttotal: 10.1ms\tremaining: 30.3s\n",
      "100:\tlearn: 5236.8856063\ttest: 6322.3461617\tbest: 6321.3850269 (96)\ttotal: 462ms\tremaining: 13.2s\n",
      "200:\tlearn: 5077.4470183\ttest: 6326.9370593\tbest: 6320.8265767 (106)\ttotal: 968ms\tremaining: 13.5s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 6320.826577\n",
      "bestIteration = 106\n",
      "\n",
      "Shrink model to first 107 iterations.\n",
      "Model 2: Fold 4\n",
      "0:\tlearn: 5362.8528598\ttest: 5176.3314639\tbest: 5176.3314639 (0)\ttotal: 5.19ms\tremaining: 15.6s\n",
      "100:\tlearn: 5196.8557805\ttest: 5195.2023825\tbest: 5175.9487918 (1)\ttotal: 476ms\tremaining: 13.7s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 5175.948792\n",
      "bestIteration = 1\n",
      "\n",
      "Shrink model to first 2 iterations.\n",
      "Model 2: Fold 5\n",
      "0:\tlearn: 5404.1619745\ttest: 4726.3471327\tbest: 4726.3471327 (0)\ttotal: 4.86ms\tremaining: 14.6s\n",
      "100:\tlearn: 5241.9506162\ttest: 4747.5781711\tbest: 4726.3471327 (0)\ttotal: 447ms\tremaining: 12.8s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4726.347133\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 2: Fold 6\n",
      "0:\tlearn: 5390.2179928\ttest: 4854.2136787\tbest: 4854.2136787 (0)\ttotal: 5.07ms\tremaining: 15.2s\n",
      "100:\tlearn: 5223.5856216\ttest: 4901.8951058\tbest: 4854.2136787 (0)\ttotal: 450ms\tremaining: 12.9s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4854.213679\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 2: Fold 7\n",
      "0:\tlearn: 5268.6212085\ttest: 5924.5171216\tbest: 5924.5171216 (0)\ttotal: 4.9ms\tremaining: 14.7s\n",
      "100:\tlearn: 5107.2787965\ttest: 5956.2257814\tbest: 5924.4244849 (1)\ttotal: 449ms\tremaining: 12.9s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 5924.424485\n",
      "bestIteration = 1\n",
      "\n",
      "Shrink model to first 2 iterations.\n",
      "Model 2: Fold 8\n",
      "0:\tlearn: 5244.8016785\ttest: 6092.0343958\tbest: 6092.0343958 (0)\ttotal: 4.82ms\tremaining: 14.4s\n",
      "100:\tlearn: 5089.1321058\ttest: 6120.8548593\tbest: 6092.0343958 (0)\ttotal: 442ms\tremaining: 12.7s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 6092.034396\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Model 2: Fold 9\n",
      "0:\tlearn: 5402.6919897\ttest: 4673.7891153\tbest: 4673.7891153 (0)\ttotal: 4.8ms\tremaining: 14.4s\n",
      "100:\tlearn: 5234.5385908\ttest: 4705.7225993\tbest: 4668.4799271 (5)\ttotal: 431ms\tremaining: 12.4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4668.479927\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n",
      "Model 2: Fold 10\n",
      "0:\tlearn: 5450.5887233\ttest: 3958.0276107\tbest: 3958.0276107 (0)\ttotal: 5.05ms\tremaining: 15.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\tlearn: 5287.2984330\ttest: 3988.4433922\tbest: 3958.0276107 (0)\ttotal: 466ms\tremaining: 13.4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 3958.027611\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n"
     ]
    }
   ],
   "source": [
    "skfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "model_1 = CatBoostRegressor(random_seed=seed*10,\n",
    "                            random_strength=3.0,\n",
    "                            l2_leaf_reg=1,\n",
    "                            n_estimators=3000,\n",
    "                            learning_rate=0.02,\n",
    "                            cat_features=categorical_feature_indices)\n",
    "    \n",
    "for fold, (trn_idx, val_idx) in enumerate(skfold.split(train_x_1, train_y)):\n",
    "        print(f'Model 1: Fold {fold + 1}')\n",
    "        X_train, y_train = train_x_1.iloc[trn_idx], train_y[trn_idx]\n",
    "        X_valid, y_valid = train_x_1.iloc[val_idx], train_y[val_idx]\n",
    "            \n",
    "        if fold == 0:\n",
    "            model_1.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_valid, y_valid),\n",
    "                verbose=100,\n",
    "                use_best_model=True,\n",
    "                early_stopping_rounds=100)    \n",
    "        else:\n",
    "            model_1.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_valid, y_valid),\n",
    "                verbose=100,\n",
    "                use_best_model=True,\n",
    "                early_stopping_rounds=100,\n",
    "                init_model=model_1)    \n",
    "        \n",
    "        \n",
    "model_2 = CatBoostRegressor(random_seed=seed*10,\n",
    "                            random_strength=3.0,\n",
    "                            l2_leaf_reg=1,\n",
    "                            n_estimators=3000,\n",
    "                            learning_rate=0.022,\n",
    "                            cat_features=categorical_feature_indices)\n",
    "    \n",
    "for fold, (trn_idx, val_idx) in enumerate(skfold.split(train_x_2, train_y)):\n",
    "        print(f'Model 2: Fold {fold + 1}')\n",
    "        X_train, y_train = train_x_2.iloc[trn_idx], train_y[trn_idx]\n",
    "        X_valid, y_valid = train_x_2.iloc[val_idx], train_y[val_idx]\n",
    "            \n",
    "        if fold == 0:\n",
    "            model_2.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_valid, y_valid),\n",
    "                verbose=100,\n",
    "                use_best_model=True,\n",
    "                early_stopping_rounds=100)    \n",
    "        else:\n",
    "            model_2.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_valid, y_valid),\n",
    "                verbose=100,\n",
    "                use_best_model=True,\n",
    "                early_stopping_rounds=100,\n",
    "                init_model=model_2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission csv\n",
    "\n",
    "The first group of models are given higher weights since they make use of the more detailed features of the different transaction types as apposed to the other that only look at the product codes and channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_model_averaging(mdl1_x, mdl2_x, weights=[1.5/2, 0.5/2]):\n",
    "    ''' Combines the models to make a weighted model average prediction.'''\n",
    "    md1_yhat = model_1.predict(mdl1_x)\n",
    "    md2_yhat = model_2.predict(mdl2_x)\n",
    "\n",
    "    return ((md1_yhat*weights[0]) +\n",
    "            (md2_yhat*weights[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "# Make the predictions\n",
    "y_hat = weighted_model_averaging(test_x_1, test_x_2)\n",
    "\n",
    "# Make an array ready for submission csv\n",
    "for prediction, userid in zip(y_hat, test_customer_identifiers):\n",
    "    pred = f'{prediction:}'\n",
    "    all_predictions.append(np.array([userid, pred]))\n",
    "\n",
    "all_predictions = np.asarray(all_predictions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission.csv in the root code directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.savetxt(\"submission.csv\", all_predictions, delimiter=\",\", fmt='%s', \n",
    "               header='CUSTOMER_IDENTIFIER,DECLARED_NET_INCOME',comments='')\n",
    "except:\n",
    "    print(\"Something went wrong.. Close the submissions.csv file ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "absa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
